{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shiyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "import copy\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility func\n",
    "\n",
    "# preprocess text\n",
    "def preprocess_text(text_str):\n",
    "    # remove punctuation and numbers\n",
    "    removed_punc_text = re.sub('[^A-Za-z]', ' ', text_str)\n",
    "    removed_punc_text_list = removed_punc_text.split()\n",
    "#     print(len(removed_punc_text_list)) #test\n",
    "\n",
    "    # remove stop words\n",
    "#     stop_words = set(stopwords.words('english'))  \n",
    "#     removed_punc_stop_text_list = [w for w in removed_punc_text_list if not w in stop_words]  \n",
    "    # print(len(removed_punc_stop_text_list)) #test\n",
    "\n",
    "    # stemming (base words)\n",
    "    stemmer = PorterStemmer()\n",
    "    for i in range(len(removed_punc_text_list)):\n",
    "        removed_punc_text_list[i] = stemmer.stem(removed_punc_text_list[i])\n",
    "    return removed_punc_text_list\n",
    "\n",
    "\n",
    "\n",
    "# calculate probability of each word in specific class\n",
    "def getProbDic(vocalbulary_dic, class_dic, total_words_num_in_class, vocabulary_length):\n",
    "    smooth_num = 0.01\n",
    "    word_prob = {} # key: word, value: probability of word, log10 format\n",
    "    for key, val in vocalbulary_dic.items():\n",
    "        fre = class_dic.get(key)\n",
    "        prob = 0 \n",
    "        # if word doesn't exist in yes_class_vocabulary, add 0.01 smooth\n",
    "        if fre == None:\n",
    "            prob = smooth_num / (total_words_num_in_class + vocabulary_length * smooth_num)\n",
    "        else:\n",
    "            prob = fre + smooth_num / (total_words_num_in_class + vocabulary_length * smooth_num)\n",
    "        word_prob[key] = math.log10(prob)\n",
    "        #print(word_prob[key])\n",
    "    return word_prob\n",
    "\n",
    "def cal_total_words(class_dic):\n",
    "    total_words = 0\n",
    "    for key, val in class_dic.items():\n",
    "        total_words += val\n",
    "    return total_words\n",
    "\n",
    "def get_score(prob_class, class_dic, document_list):\n",
    "    score = prob_class\n",
    "    for word in document_list:\n",
    "        if class_dic.get(word) != None:\n",
    "            score += class_dic.get(word)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "# read specific columns from csv file\n",
    "df = pd.read_csv(\"covid_training.csv\", usecols = ['tweet_id','text', 'q1_label'])\n",
    "# print(df)\n",
    "\n",
    "# count the number of documents in each class (yes/no)\n",
    "# read text in each row of training set\n",
    "yes_class_num = 0\n",
    "no_class_num = 0\n",
    "total_class_num = 0\n",
    "\n",
    "yes_class_text = \"\"\n",
    "no_class_text = \"\"\n",
    "all_text = \"\" # concatenate all texts to one string\n",
    "\n",
    "for i, col in df.iterrows():\n",
    "    if col['q1_label'].lower() == \"yes\":\n",
    "        yes_class_num += 1\n",
    "        yes_class_text += (col['text'].lower() + \" \")\n",
    "    else:\n",
    "        no_class_num += 1\n",
    "        no_class_text += (col['text'].lower() + \" \")\n",
    "    all_text += (col['text'].lower() + \" \")\n",
    "    total_class_num += 1\n",
    "\n",
    "# print(all_text) #test\n",
    "\n",
    "# probability of each class\n",
    "prob_class_yes = math.log10(yes_class_num / total_class_num)\n",
    "prob_class_no = math.log10(no_class_num / total_class_num)\n",
    "\n",
    "print(yes_class_num)\n",
    "print(no_class_num)\n",
    "########################## preprocess all_text\n",
    "\n",
    "# map punctuation to space\n",
    "# translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) \n",
    "\n",
    "# generate vocabulary \n",
    "# count words frequency in all_text\n",
    "# remove punctuation and quotation marks\n",
    "# removed_punc_text = all_text.translate(translator).replace(\"'\",\" \").replace(\"“\",\" \").replace(\"”\",\" \").replace(\"’\",\" \").replace(\"‘\",\" \")\n",
    "all_text_list = preprocess_text(all_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################# original_vocabulary #################\n",
    "\n",
    "# calculate words frequency for original_vocabulary\n",
    "original_vocabulary = dict(Counter(all_text_list))\n",
    "# print(original_vocabulary)\n",
    "\n",
    "# count total vocabulary words in all_text based on original vocabulary\n",
    "abs_ov_length = len(original_vocabulary)\n",
    "# print(abs_ov_length)\n",
    "\n",
    "# test\n",
    "# print(original_vocabulary.get(\"the\")) # should be None\n",
    "\n",
    "# test\n",
    "abs_ov_length_dupli = cal_total_words(original_vocabulary)\n",
    "# print(abs_ov_length_dupli)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# count words frequency in yes_class_text based on original_vocabulary\n",
    "# removed_punc_yes_class_text = yes_class_text.translate(translator).replace(\"'\",\" \").replace(\"“\",\" \").replace(\"”\",\" \").replace(\"’\",\" \").replace(\"‘\",\" \")\n",
    "yes_class_text_list = preprocess_text(yes_class_text)\n",
    "\n",
    "# calculate words frequency\n",
    "yes_class_vocabulary = dict(Counter(yes_class_text_list))\n",
    "# print(yes_class_vocabulary)\n",
    "\n",
    "total_words_in_yes_class_text_ov = cal_total_words(yes_class_vocabulary)\n",
    "# print(total_words_in_yes_class_text_ov)\n",
    "\n",
    "# calculate probability of each word in class yes\n",
    "word_prob_yes_class = getProbDic(original_vocabulary, yes_class_vocabulary, total_words_in_yes_class_text_ov, abs_ov_length)\n",
    "# print(word_prob_yes_class[\"the\"]) # test\n",
    "\n",
    "\n",
    "\n",
    "# count words frequency in no_class_text based on original_vocabulary\n",
    "# removed_punc_no_class_text = no_class_text.translate(translator).replace(\"'\",\" \").replace(\"“\",\" \").replace(\"”\",\" \").replace(\"’\",\" \").replace(\"‘\",\" \")\n",
    "no_class_text_list = preprocess_text(no_class_text)\n",
    "\n",
    "# calculate words frequency\n",
    "no_class_vocabulary = dict(Counter(no_class_text_list))\n",
    "# print(no_class_vocabulary)\n",
    "\n",
    "total_words_in_no_class_text_ov = cal_total_words(no_class_vocabulary)\n",
    "# print(total_words_in_no_class_text_ov)\n",
    "\n",
    "# calculate probability of each word in class no\n",
    "word_prob_no_class = getProbDic(original_vocabulary, no_class_vocabulary, total_words_in_no_class_text_ov, abs_ov_length)\n",
    "# print(word_prob_no_class[\"the\"]) # test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'for': 125, 'the': 461, 'american': 15, 'best': 6, 'way': 13, 'to': 338, 'tell': 9, 'if': 43, 'you': 120, 'have': 66, 'covid': 183, 'is': 204, 'cough': 7, 'in': 187, 'a': 279, 'person': 13, 's': 122, 'face': 8, 'and': 228, 'wait': 4, 'their': 28, 'test': 60, 'result': 4, 'thi': 128, 'fuck': 12, 'can': 49, 'y': 16, 'all': 55, 'pleas': 26, 'just': 38, 'follow': 10, 'govern': 18, 'instruct': 2, 'so': 28, 'we': 81, 'knock': 2, 'out': 26, 'be': 92, 'done': 6, 'i': 116, 'feel': 8, 'like': 34, 'that': 117, 'keep': 7, 'lose': 3, 'more': 20, 'time': 27, 'becaus': 20, 'one': 24, 'or': 34, 'two': 10, 'kid': 5, 't': 275, 'direct': 3, 'no': 32, 'but': 41, 'corona': 91, 'viru': 85, 'disappear': 2, 'befor': 5, 'april': 2, 'actual': 7, 'suck': 2, 'of': 231, 'someon': 7, 'who': 43, 'spent': 3, 'hour': 2, 'protect': 12, 'move': 2, 'critic': 4, 'ill': 7, 'patient': 18, 'around': 9, 'are': 77, 'onli': 17, 'at': 53, 'start': 9, 'am': 8, 'peopl': 62, 'do': 32, 'social': 9, 'distanc': 2, 'self': 10, 'isol': 5, 'http': 187, 'co': 186, 'on': 99, 'my': 42, 'door': 4, 'told': 9, 'him': 13, 'he': 45, 'stay': 10, 'free': 4, 'month': 8, 'employ': 2, 'with': 75, 'don': 26, 'need': 26, 'bori': 3, 'me': 21, 'what': 36, 'moral': 2, 'correct': 4, 'someth': 9, 'after': 20, 'over': 25, 'better': 4, 'never': 10, 'hear': 10, 'anyon': 6, 'end': 7, 'worker': 14, 'again': 10, 'those': 14, 'gener': 6, 'food': 4, 'employe': 2, 'didn': 8, 'even': 8, 'think': 14, 'deserv': 2, 'surviv': 4, 'dr': 5, 'past': 5, 'week': 28, 'treat': 4, 'pakistan': 4, 'knew': 6, 'there': 36, 'wa': 42, 'ppe': 3, 'today': 15, 'lost': 5, 'hi': 22, 'own': 6, 'battl': 3, 'coronaviru': 128, 'gave': 3, 'life': 10, 'hope': 8, 'mani': 15, 'know': 23, 'name': 2, 'fun': 4, 'fact': 9, 'it': 124, 'tradit': 3, 'spread': 47, 'fatal': 2, 'diseas': 10, 'everi': 11, 'other': 20, 'countri': 16, 'not': 74, 'fulli': 2, 'by': 47, 'white': 5, 'amp': 45, 'take': 20, 'account': 8, 'devast': 2, 'live': 16, 'when': 22, 'shit': 3, 'pass': 5, 'promis': 4, 'each': 2, 're': 19, 'go': 30, 'our': 23, 'short': 5, 'breath': 4, 'symptom': 15, 'right': 12, 'also': 10, 'give': 13, 'anxieti': 3, 'ha': 54, 'been': 20, 'last': 9, 'pneumonia': 3, 'young': 4, 'aren': 2, 'risk': 5, 'they': 41, 'll': 7, 'mild': 2, 'wrong': 2, 'want': 22, 'open': 2, 'up': 24, 'about': 45, 've': 6, 'gone': 2, 'through': 9, 'these': 10, 'day': 24, 'icu': 3, 'phd': 2, 'from': 65, 'whatsapp': 7, 'univers': 3, 'friend': 6, 'then': 11, 'cool': 2, 'm': 16, 'stock': 8, 'against': 12, 'relief': 4, 'bill': 7, 'futur': 2, 'hey': 3, 'grandpa': 3, 'korea': 5, 'develop': 6, 'minut': 2, 'kit': 10, 'now': 37, 'ramp': 2, 'product': 3, 'plan': 3, 'export': 2, 'rt': 11, 'suicid': 2, 'big': 9, 'room': 4, 'isn': 12, 'epidem': 4, 'happen': 6, 'come': 14, 'bad': 6, 'ever': 3, 'good': 7, 'health': 30, 'gonna': 6, 'see': 15, 'celebr': 3, 'tri': 8, 'sing': 2, 'away': 9, 'movi': 4, 'mayb': 3, 'will': 37, 'leav': 7, 'thread': 10, 'mostli': 2, 'talk': 4, 'here': 19, 'goe': 3, 'current': 4, 'make': 22, 'surgic': 2, 'mask': 9, 'medic': 13, 'suppli': 3, 'million': 9, 'look': 16, 'help': 19, 'ani': 14, 'possibl': 5, 'dure': 8, 'n': 9, 'e': 32, 'man': 11, 'hous': 5, 'would': 15, 'pop': 2, 'thursday': 4, 'cure': 6, 'morn': 5, 'did': 12, 'almost': 4, 'everyth': 4, 'work': 27, 'home': 19, 'saw': 6, 'had': 16, 'beer': 2, 'an': 32, 'outsid': 7, 'either': 4, 'tabl': 2, 'air': 2, 'contact': 6, 'wash': 8, 'hand': 14, 'still': 14, 'caught': 3, 'thing': 13, 'understand': 5, 'how': 21, 'year': 17, 'order': 8, 'system': 7, 'bro': 3, 'china': 21, 'read': 12, 'get': 30, 'adequ': 2, 'invit': 3, 'sometim': 2, 'ass': 5, 'miss': 5, 'your': 34, 'without': 10, 'stop': 27, 'said': 9, 'updat': 2, 'compil': 2, 'italian': 2, 'mayor': 2, 'violat': 2, 'quarantin': 20, 'ye': 3, 'subtitl': 2, 'accur': 3, 'v': 7, 'unless': 3, 'doctor': 14, 'both': 7, 'drug': 2, 'affect': 2, 'heart': 3, 'lead': 5, 'death': 21, 'med': 2, 'condit': 3, 'speak': 2, 'into': 7, 'exist': 2, 'thank': 11, 'everyon': 13, 'messag': 9, 'support': 6, 'encourag': 2, 'ask': 13, 'er': 7, 'share': 13, 'doc': 5, 'brief': 5, 'fellow': 2, 'citizen': 7, 'absolut': 4, 'panic': 12, 'essenti': 2, 'etc': 2, 'avail': 2, 'centr': 2, 'state': 15, 'close': 13, 'ensur': 2, 'togeth': 3, 'fight': 14, 'creat': 5, 'india': 18, 'found': 6, 'q': 9, 'clean': 4, 'gaston': 3, 'whi': 16, 'haven': 3, 'minim': 4, 'prevent': 6, 'dysfunct': 2, 'x': 3, 'new': 20, 'region': 2, 'presid': 14, 'graduat': 2, 'parti': 8, 'send': 4, 'polic': 3, 'final': 2, 'some': 9, 'sensibl': 2, 'advic': 5, 'nurs': 5, 'seen': 4, 'lot': 4, 'recommend': 3, 'avoid': 5, 'first': 17, 'place': 4, 'real': 7, 'insid': 6, 'chang': 5, 'dad': 2, 'p': 4, 'z': 2, 'folk': 4, 'love': 3, 'call': 22, 'under': 7, 'millenni': 3, 'break': 7, 'yell': 2, 'boomer': 2, 'parent': 2, 'won': 7, 'sit': 2, 'fear': 6, 'guy': 7, 'happi': 2, 'wow': 3, 'world': 16, 'organ': 2, 'warn': 7, 'could': 11, 'put': 16, 'hospit': 18, 'kill': 8, 'sick': 15, 'where': 6, 'differ': 3, 'between': 3, 'els': 5, 'much': 6, 'video': 6, 'spain': 4, 'watch': 8, 'cancel': 5, 'check': 8, 'doesn': 3, 'as': 48, 'hard': 4, 'child': 3, 'came': 5, 'himself': 2, 'might': 3, 'remind': 10, 'cdc': 3, 'announc': 8, 'via': 2, 'stori': 6, 'challeng': 4, 'down': 9, 'save': 3, 'crisi': 11, 'voic': 4, 'healthcar': 4, 'pay': 4, 'statu': 2, 'servic': 2, 'aggress': 3, 'issu': 4, 'them': 17, 'h': 2, 'mom': 3, 'her': 11, 'g': 5, 'bt': 2, 'son': 2, 'min': 3, 'continu': 7, 'black': 3, 'rona': 2, 'may': 7, 'taken': 3, 'vega': 3, 'empti': 2, 'tbh': 2, 'nasti': 2, 'flu': 20, 'went': 5, 'round': 3, 'earli': 8, 'jan': 4, 'ppl': 4, 'were': 13, 'includ': 5, 'due': 11, 'thought': 5, 'twitter': 6, 'prom': 3, 'further': 2, 'th': 3, 'march': 9, 'interact': 2, 'join': 4, 'yet': 6, 'war': 2, 'politician': 4, 'act': 2, 'than': 18, 'itali': 8, 'rememb': 3, 'job': 3, 'compani': 5, 'regard': 2, 'global': 9, 'situat': 3, 'gummi': 2, 'ago': 11, 'entir': 7, 'popul': 3, 'bitch': 5, 'origin': 6, 'back': 10, 'mean': 3, 'lq': 2, 'eat': 2, 'pure': 3, 'ain': 3, 'built': 2, 'same': 6, 'epidemiologist': 2, 'say': 24, 'trump': 31, 'handl': 6, 'most': 5, 'irrespons': 3, 'elect': 3, 'offici': 12, 'use': 7, 'among': 2, 'public': 17, 'expert': 10, 'approach': 2, 'top': 3, 'k': 8, 'case': 48, 'worldwid': 3, 'senat': 3, 'trade': 4, 'should': 15, 'immedi': 4, 'resign': 4, 'word': 2, 'donat': 10, 'idk': 2, 'small': 2, 'alway': 5, 'park': 2, 'spot': 2, 'until': 2, 'profession': 3, 'hero': 3, 'line': 4, 'alreadi': 8, 'front': 5, 'us': 32, 'dear': 12, 'allah': 2, 'cost': 7, 'show': 7, 'too': 7, 'long': 3, 'fat': 3, 'took': 4, 'off': 6, 'dirti': 2, 'racist': 4, 'bodi': 2, 'beat': 2, 'lord': 2, 'wear': 4, 'hit': 4, 'find': 3, 'she': 9, 'infect': 19, 'harvey': 2, 'germani': 3, 'total': 3, 'ventil': 2, 'made': 5, 'govt': 2, 'bed': 2, 'rate': 7, 'tini': 2, 'bco': 2, 'mass': 3, 'allow': 5, 'fewer': 2, 'non': 2, 'report': 15, 'age': 5, 'die': 27, 'believ': 6, 'la': 6, 'nation': 7, 'indiafightscorona': 2, 'increas': 3, 'realdonaldtrump': 22, 'd': 11, 'shut': 3, 'scienc': 6, 'evid': 3, 'polit': 6, 'older': 2, 'great': 7, 'coupl': 2, 'whatev': 2, 'next': 4, 'admit': 8, 'whole': 2, 'within': 2, 'dead': 2, 'school': 6, 'legitim': 2, 'huge': 2, 'provid': 5, 'strong': 2, 'desir': 2, 'return': 9, 'function': 2, 'economi': 3, 'such': 5, 'societi': 3, 'citi': 5, 'corpor': 2, 'bailout': 2, 'suffici': 2, 'care': 14, 'class': 7, 'famili': 7, 'focu': 3, 'street': 2, 'wouldn': 2, 'proper': 2, 'treatment': 3, 'got': 9, 'old': 9, 'commun': 4, 'learn': 4, 'director': 2, 'allegedli': 8, 'posit': 9, 'refus': 10, 'priorit': 3, 'respect': 3, 'clear': 3, 'sen': 4, 'illeg': 2, 'inform': 13, 'buy': 3, 'profit': 2, 'texa': 2, 'ny': 3, 'largest': 2, 'fake': 35, 'question': 4, 'tweet': 11, 'rumor': 14, 'let': 12, 'news': 23, 'guard': 2, 'respons': 11, 'link': 15, 'stuff': 2, 'natur': 2, 'larg': 2, 'sourc': 6, 'control': 4, 'page': 2, 'misinform': 4, 'trust': 5, 'republican': 4, 'implic': 2, 'sell': 2, 'while': 7, 'hoax': 28, 'fox': 4, 'claim': 6, 'receipt': 2, 'committe': 3, 'danger': 4, 'note': 4, 'imag': 2, 'platform': 4, 'caus': 7, 'disregard': 3, 'pm': 6, 'local': 4, 'wuhan': 5, 'few': 4, 'light': 3, 'number': 11, 'reportedli': 5, 'uk': 5, 'border': 2, 'johnson': 2, 'shutdown': 2, 'refer': 2, 'polici': 2, 'failur': 2, 'leadership': 5, 'gop': 2, 'outbreak': 11, 'blame': 5, 'busi': 5, 'enough': 8, 'chief': 5, 'staff': 4, 'kyari': 2, 'suspect': 2, 'contract': 2, 'serious': 3, 'sinc': 9, 'trip': 4, 'ron': 3, 'paul': 4, 'publish': 3, 'articl': 4, 'rumour': 3, 'less': 4, 'list': 3, 'onlin': 2, 'flight': 3, 'sure': 4, 'info': 4, 'hold': 4, 'daili': 5, 'cdcgov': 3, 'latest': 5, 'step': 2, 'leader': 2, 'pandem': 7, 'america': 5, 'fill': 2, 'sorri': 3, 'noth': 5, 'seriou': 5, 'bigot': 2, 'delhi': 2, 'indian': 5, 'woman': 4, 'belong': 2, 'area': 7, 'north': 2, 'night': 2, 'u': 6, 'worri': 2, 'spoke': 2, 'photo': 3, 'circul': 3, 'fals': 9, 'detail': 5, 'confirm': 14, 'nigeria': 3, 'paid': 3, 'c': 3, 'congress': 4, 'propaganda': 2, 'shortag': 2, 'student': 3, 'occur': 2, 'viewer': 2, 'action': 6, 'conspiraci': 5, 'realli': 9, 'januari': 5, 'lv': 2, 'human': 2, 'pl': 2, 'forward': 2, 'anim': 2, 'saliva': 2, 'educ': 3, 'yourself': 3, 'urg': 2, 'spray': 3, 'throughout': 2, 'nytim': 2, 'explain': 2, 'fakenew': 8, 'veri': 4, 'pidi': 2, 'quot': 2, 'chines': 10, 'communist': 4, 'purpos': 2, 'activ': 2, 'push': 2, 'theori': 3, 'campaign': 3, 'unit': 7, 'singl': 3, 'patna': 5, 'problem': 5, 'qatar': 13, 'st': 3, 'senior': 2, 'hiv': 2, 'aid': 2, 'immun': 2, 'scam': 2, 'upon': 2, 'letter': 2, 'post': 2, 'clearli': 2, 'far': 4, 'standard': 2, 'hurt': 2, 'nobodi': 2, 'viral': 4, 'media': 16, 'demand': 2, 'impact': 2, 'w': 13, 'must': 4, 'second': 2, 'unintent': 2, 'feder': 3, 'sever': 3, 'respiratori': 3, 'j': 5, 'justic': 2, 'parliament': 2, 'emerg': 11, 'rule': 2, 'suspend': 2, 'prison': 2, 'airlin': 2, 'instead': 7, 'shame': 3, 'hate': 2, 'well': 6, 'reason': 2, 'anoth': 6, 'peddl': 2, 'vegetarian': 4, 'date': 2, 'uganda': 2, 'true': 4, 'team': 2, 'fraud': 2, 'alert': 5, 'taxpay': 2, 'websit': 2, 'impos': 3, 'financi': 3, 'advis': 2, 'drtedro': 3, 'full': 4, 'extent': 2, 'disast': 2, 'fine': 5, 'ban': 3, 'uae': 3, 'heard': 4, 'recov': 3, 'game': 2, 'wish': 3, 'pibfactcheck': 2, 'releas': 2, 'tie': 3, 'threaten': 2, 'comment': 2, 'request': 2, 'delet': 2, 'democrat': 3, 'soon': 5, 'expos': 3, 'awar': 4, 'blood': 2, 'forev': 2, 'stain': 2, 'precaut': 4, 'sensation': 2, 'safe': 4, 'lie': 6, 'gather': 2, 'donald': 3, 'liber': 2, 'answer': 2, 'repeat': 2, 'distribut': 3, 'gen': 2, 'attend': 2, 'hotel': 3, 'decid': 7, 'myself': 2, 'period': 4, 'disinform': 2, 'hatr': 2, 'doe': 5, 'season': 6, 'turn': 4, 'fail': 6, 'cnn': 2, 'tapper': 2, 'o': 3, 'play': 3, 'malaysian': 2, 'b': 3, 'quiet': 2, 'cannot': 3, 'arrest': 2, 'bihar': 5, 'travel': 17, 'histori': 5, 'aiim': 5, 'foreign': 2, 'prepar': 4, 'migrant': 2, 'singapor': 3, 'south': 4, 'africa': 2, 'saif': 2, 'ali': 2, 'reach': 2, 'member': 3, 'scare': 6, 'oh': 2, 'coronavirusupd': 3, 'rwanda': 2, 'bring': 2, 'usa': 3, 'fifacom': 3, 'accept': 3, 'fifaworldcup': 3, 'fr': 2, 'amnesti': 2, 'milan': 2, 'custom': 2, 'money': 4, 'glove': 3, 'plastic': 2, 'bag': 2, 'tonight': 6, 'collect': 4, 'safeti': 5, 'victim': 3, 'suggest': 2, 'combin': 3, 'sar': 5, 'damag': 4, 'lung': 5, 'thespybrief': 2, 'nationalnurs': 2, 'statement': 5, 'facil': 2, 'west': 2, 'neg': 3, 'album': 3, 'box': 2, 'cover': 3, 'build': 2, 'deadli': 3, 'incompet': 5, 'administr': 6, 'massiv': 2, 'potu': 5, 'contain': 12, 'friday': 2, 'coronavirusoutbreak': 5, 'vaccin': 3, 'f': 2, 'lack': 6, 'derelict': 3, 'duti': 3, 'spew': 2, 'downplay': 3, 'unaccept': 2, 'rick': 3, 'market': 2, 'estim': 2, 'assist': 3, 'secretari': 2, 'probabl': 2, 'asian': 8, 'individu': 3, 'transmiss': 3, 'toilet': 2, 'paper': 3, 'ff': 2, 'center': 2, 'vs': 2, 'becom': 2, 'catch': 2, 'data': 2, 'research': 5, 'retweet': 3, 'meghan': 2, 'joke': 2, 'accord': 4, 'which': 7, 'tactic': 2, 'vp': 2, 'penc': 5, 'below': 3, 'sneez': 3, 'across': 3, 'gt': 2, 'preval': 2, 'appar': 2, 'measur': 4, 'benefit': 4, 'franc': 5, 'enter': 2, 'tiktok': 4, 'least': 4, 'racism': 2, 'touch': 3, 'stake': 2, 'ticket': 2, 'vietnam': 2, 'four': 2, 'consult': 2, 'translat': 2, 'cold': 3, 'clip': 2, 'coronviru': 2, 'write': 2, 'bunch': 2, 'deadlier': 3, 'attempt': 2, 'enemi': 3, 'requir': 2, 'effort': 3, 'combat': 4, 'ir': 2, 'holi': 3, 'insur': 2, 'doh': 2, 'initi': 2, 'approx': 3, 'dynasti': 2, 'point': 2, 'hongkong': 2, 'listen': 2, 'littl': 2, 'clinic': 4, 'offic': 4, 'seattl': 2, 'confront': 3, 'fund': 2, 'privat': 2, 'statedept': 2, 'houseforeign': 2, 'secpompeo': 2, 'deni': 5, 'strang': 2, 'iran': 3, 'japan': 2, 'hunch': 2, 'enabl': 2, 'empir': 2, 'visit': 3, 'known': 2, 'incl': 2, 'jordan': 2, 'russia': 2, 'wors': 2, 'kag': 2, 'three': 2, 'wonder': 4, 'desist': 2, 'ag': 2, 'salari': 4, 'hhsgov': 2, 'herea': 4, 'road': 2, 'crash': 2, 'chronic': 2, 'novel': 2, 'henc': 2, 'minist': 3, 'israel': 4, 'netanyahu': 3, 'isra': 3, 'namast': 2, 'coronaoutbreak': 3, 'coronaalert': 2, 'diagnos': 3, 'ebola': 3, 'effect': 2, 'associ': 3, 'access': 2, 'fisa': 2, 'peoplea': 2, 'wona': 2, 'quit': 2, 'coronavirusindia': 2, 'apolog': 2, 'excel': 2, 'mr': 2, 'doesna': 3, 'amaz': 2, 'feb': 3, 'wake': 2, 'freak': 2, 'sanit': 2, 'cheap': 2, 'arriv': 2, 'incub': 2, 'throat': 2, 'cpac': 2, 'attende': 3, 'gov': 3, 'part': 2, 'kind': 2, 'pag': 2, 'seem': 3, 'focus': 2, 'taal': 2, 'cov': 2, 'youa': 2, 'ng': 2, 'na': 2, 'chloroquin': 2, 'anti': 3}\n",
      "11658\n"
     ]
    }
   ],
   "source": [
    "################# filtered_vocabulary #################\n",
    "# calculate words frequency for filtered_vocabulary\n",
    "filtered_vocabulary = {key:val for key, val in original_vocabulary.items() if val != 1}\n",
    "print(filtered_vocabulary)\n",
    "\n",
    "# count total vocabulary words in all_text based on filtered vocabulary\n",
    "abs_fv_length = len(filtered_vocabulary)\n",
    "\n",
    "# test\n",
    "abs_fv_length_dupli = cal_total_words(filtered_vocabulary)\n",
    "print(abs_fv_length_dupli)\n",
    "\n",
    "# calculate class_dic based on filtered_vocabulary\n",
    "yes_class_vocabulary_fv = copy.deepcopy(yes_class_vocabulary)\n",
    "for key in yes_class_vocabulary.keys():\n",
    "    # remove all words that doesn't exist in filtered_vocabulary\n",
    "    if filtered_vocabulary.get(key) == None:\n",
    "        del yes_class_vocabulary_fv[key]\n",
    "\n",
    "\n",
    "total_words_in_yes_class_text_fv = cal_total_words(yes_class_vocabulary_fv)\n",
    "# print(total_words_in_yes_class_text_fv)\n",
    "\n",
    "# calculate probability of each word in class yes\n",
    "word_prob_yes_class_fv = getProbDic(filtered_vocabulary, yes_class_vocabulary_fv, total_words_in_yes_class_text_fv, abs_fv_length)\n",
    "\n",
    "\n",
    "\n",
    "# calculate class_dic based on filtered_vocabulary\n",
    "no_class_vocabulary_fv = copy.deepcopy(no_class_vocabulary)\n",
    "for key in no_class_vocabulary.keys():\n",
    "    # remove all words that doesn't exist in filtered_vocabulary\n",
    "    if filtered_vocabulary.get(key) == None:\n",
    "        del no_class_vocabulary_fv[key]\n",
    "\n",
    "\n",
    "total_words_in_no_class_text_fv = cal_total_words(no_class_vocabulary_fv)\n",
    "# print(total_words_in_no_class_text_fv)\n",
    "\n",
    "# calculate probability of each word in class no\n",
    "word_prob_no_class_fv = getProbDic(filtered_vocabulary, no_class_vocabulary_fv, total_words_in_no_class_text_fv, abs_fv_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes_score 50.76936429128407\n",
      "no_score 22.007053941359086\n",
      "yes_score 27.096460943270895\n",
      "no_score 8.818093223773866\n",
      "yes_score 63.076108958394336\n",
      "no_score 16.84303550676065\n",
      "yes_score 27.717620484658678\n",
      "no_score -15.157223130234389\n",
      "yes_score 29.07271844963968\n",
      "no_score -4.745997818985202\n",
      "yes_score 28.77961663251946\n",
      "no_score 14.730287692523168\n",
      "yes_score 20.09783331813091\n",
      "no_score -5.423973736058697\n",
      "yes_score 53.30029722082114\n",
      "no_score 4.563222865601342\n",
      "yes_score 38.44687400285244\n",
      "no_score 16.564038513660467\n",
      "yes_score 21.724014337543313\n",
      "no_score 13.495268955542207\n",
      "yes_score 22.310439221097624\n",
      "no_score 3.2146343330615688\n",
      "yes_score 10.194101321461817\n",
      "no_score 7.922919980481746\n",
      "yes_score 71.3501505966141\n",
      "no_score 39.00457824459602\n",
      "yes_score 26.44591075546791\n",
      "no_score 1.082515834298138\n",
      "yes_score 5.67661371703193\n",
      "no_score -1.2429026192451256\n",
      "yes_score 67.57240398625967\n",
      "no_score -6.88071325628575\n",
      "yes_score 19.46766377516145\n",
      "no_score -1.327228659236611\n",
      "yes_score 41.82584147209753\n",
      "no_score -8.575755101710179\n",
      "yes_score 32.03628352272755\n",
      "no_score 6.463468487357609\n",
      "yes_score 21.523240678996032\n",
      "no_score 11.291136786767241\n",
      "yes_score 30.296181020728994\n",
      "no_score -22.095076401491784\n",
      "yes_score 7.267396043983913\n",
      "no_score 4.153652583900305\n",
      "yes_score 45.93088310689116\n",
      "no_score 25.87893721510478\n",
      "yes_score 20.870177060841968\n",
      "no_score 17.397712348326948\n",
      "yes_score 31.279395672563613\n",
      "no_score 18.033896678551688\n",
      "yes_score 56.45644274609787\n",
      "no_score 9.889887587600315\n",
      "yes_score 45.84596973352745\n",
      "no_score 13.543764237350093\n",
      "yes_score 37.55153625422208\n",
      "no_score 7.1140382893378735\n",
      "yes_score 55.113357007970976\n",
      "no_score 6.708155961493895\n",
      "yes_score 21.7604907299445\n",
      "no_score 8.788361851629892\n",
      "yes_score 15.874024866708078\n",
      "no_score 13.462118023441782\n",
      "yes_score 14.153259898840764\n",
      "no_score 15.700296525796002\n",
      "yes_score 15.905923481018082\n",
      "no_score 17.322874434336754\n",
      "yes_score 5.40809471128558\n",
      "no_score 4.335418422389563\n",
      "yes_score 25.84385064826964\n",
      "no_score 0.7814853047450678\n",
      "yes_score 11.151322928043683\n",
      "no_score -2.3464648341551224\n",
      "yes_score 57.1118602235589\n",
      "no_score 10.731130212651541\n",
      "yes_score 19.704560599129653\n",
      "no_score 5.9957847871964844\n",
      "yes_score 8.380646232033763\n",
      "no_score -5.413014683777164\n",
      "yes_score 24.716522232646053\n",
      "no_score 16.86412222881991\n",
      "yes_score 13.828306237513923\n",
      "no_score -19.70582139125174\n",
      "yes_score 34.17183890733015\n",
      "no_score -28.455029497440222\n",
      "yes_score 42.95986383550183\n",
      "no_score 1.073980009838273\n",
      "yes_score 46.32639637413532\n",
      "no_score -1.0935075367022604\n",
      "yes_score 17.63981027356323\n",
      "no_score 16.178651456778017\n",
      "yes_score 42.39707224232792\n",
      "no_score 9.2662700104453\n",
      "yes_score 25.688521192639907\n",
      "no_score 17.908888033177597\n",
      "yes_score 36.41296234099259\n",
      "no_score 20.92504399780282\n",
      "yes_score 18.52393855029816\n",
      "no_score -4.034966421344166\n",
      "yes_score 60.61945975637645\n",
      "no_score 21.99711164412609\n",
      "yes_score 61.65477336950516\n",
      "no_score 29.74536963925405\n",
      "yes_score 25.333871713724605\n",
      "no_score 1.0421349219377585\n",
      "yes_score 48.07796414930501\n",
      "no_score 12.441743641351186\n",
      "yes_score 23.153123208331554\n",
      "no_score 28.023018307037702\n",
      "yes_score 51.13270770521158\n",
      "no_score 17.68611229253579\n",
      "55\n",
      "34\n",
      "21\n",
      "52\n",
      "3\n",
      " \n",
      "32\n",
      "2\n",
      " \n",
      "33\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "################# original_vocabulary #################\n",
    "\n",
    "# read specific columns from csv file\n",
    "df_test = pd.read_csv(\"covid_test_public.csv\", usecols = ['tweet_id','text', 'q1_label'])\n",
    "\n",
    "total_instances = 0\n",
    "num_of_correct_prediction = 0\n",
    "num_of_wrong_prediction = 0\n",
    "num_of_yes_predicted = 0\n",
    "num_of_no_predicted = 0\n",
    "positive_yes = 0\n",
    "positive_no = 0\n",
    "true_yes = 0\n",
    "true_no = 0\n",
    "\n",
    "\n",
    "# generate trace file based on model\n",
    "f = open(\"_NB-BOW-OV.txt\",\"w\") \n",
    "for i, col in df_test.iterrows():\n",
    "    f.write(str(col['tweet_id'])+ \"  \") \n",
    "\n",
    "    # calculate the score in each class\n",
    "    document = col['text'].lower()\n",
    "    word_list = preprocess_text(document)\n",
    "    total_instances += 1\n",
    "    # print(word_list, \"\\n\")\n",
    "\n",
    "    yes_score = get_score(prob_class_yes, word_prob_yes_class, word_list)\n",
    "    no_score = get_score(prob_class_no, word_prob_no_class, word_list)\n",
    "    print(\"yes_score\", yes_score)\n",
    "    print(\"no_score\", no_score)\n",
    "\n",
    "    if yes_score > no_score:\n",
    "        num_of_yes_predicted += 1\n",
    "        f.write(\"yes  \"+ '{:.5E}'.format(yes_score) + \"  \"+col['q1_label']+\"  \")\n",
    "        if col['q1_label'] == \"yes\":\n",
    "            f.write(\"correct\\n\")\n",
    "            num_of_correct_prediction += 1\n",
    "            positive_yes += 1\n",
    "            true_yes +=1\n",
    "        else:\n",
    "            f.write(\"wrong\\n\")\n",
    "            num_of_wrong_prediction += 1\n",
    "            true_no += 1\n",
    "    else:\n",
    "        num_of_no_predicted += 1\n",
    "        f.write(\"no  \"+ '{:.5E}'.format(no_score) + \"  \"+col['q1_label']+\"  \")\n",
    "        if col['q1_label'] == \"no\":\n",
    "            f.write(\"correct\\n\")\n",
    "            num_of_correct_prediction += 1\n",
    "            positive_no += 1\n",
    "            true_no += 1\n",
    "        else:\n",
    "            f.write(\"wrong\\n\")\n",
    "            num_of_wrong_prediction += 1\n",
    "            true_yes += 1\n",
    "\n",
    "f.close()\n",
    "\n",
    "# print(total_instances)\n",
    "# print(num_of_correct_prediction)\n",
    "# print(num_of_wrong_prediction)\n",
    "# print(num_of_yes_predicted)\n",
    "# print(num_of_no_predicted)\n",
    "# print(\" \")\n",
    "# print(positive_yes)\n",
    "# print(positive_no)\n",
    "# print(\" \")\n",
    "# print(true_yes)\n",
    "# print(true_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputting evaluation file for original vocabulary\n",
    "\n",
    "accuracy = round(num_of_correct_prediction/total_instances,4)\n",
    "precision_yes = round(positive_yes/num_of_yes_predicted,4)\n",
    "precision_no = round(positive_no/num_of_no_predicted,4)\n",
    "recall_yes = round(positive_yes/true_yes,4)\n",
    "recall_no = round(positive_no/true_no,4)\n",
    "f1_yes = round(((1**2+1)*precision_yes*recall_yes)/(1**2*precision_yes+recall_yes),4)\n",
    "f1_no = round(((1**2+1)*precision_no*recall_no)/(1**2*precision_no+recall_no),4)\n",
    "\n",
    "eval = open(\"eval_NB-BOW-OV.txt\",\"w\")\n",
    "eval.write(str(accuracy) + \"\\n\")\n",
    "eval.write(str(precision_yes) + \"  \" +str(precision_no)+\"\\n\")\n",
    "eval.write(str(recall_yes) + \"  \" +str(recall_no)+\"\\n\")\n",
    "eval.write(str(f1_yes) + \"  \" +str(f1_no)+\"\\n\")\n",
    "eval.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes_score 50.76936411365173\n",
      "no_score 27.745082571459726\n",
      "yes_score 27.096461044413562\n",
      "no_score 8.94681808517889\n",
      "yes_score 69.04785988748006\n",
      "no_score 33.86403148863605\n",
      "yes_score 27.717618929727458\n",
      "no_score 7.408714539480215\n",
      "yes_score 29.07271806287112\n",
      "no_score 1.0563924633099118\n",
      "yes_score 34.75136797707873\n",
      "no_score 25.948893562786832\n",
      "yes_score 20.157201895551463\n",
      "no_score -5.166524281541942\n",
      "yes_score 53.3002972625862\n",
      "no_score 10.494338761829816\n",
      "yes_score 38.50624181073952\n",
      "no_score 27.847008106096222\n",
      "yes_score 27.755134564959896\n",
      "no_score 19.168934049984383\n",
      "yes_score 28.282190495021865\n",
      "no_score 14.497602222117537\n",
      "yes_score 10.194101338532587\n",
      "no_score 7.922920074939068\n",
      "yes_score 71.35015035162105\n",
      "no_score 44.74260680205152\n",
      "yes_score 26.445909979810992\n",
      "no_score 12.365484870746963\n",
      "yes_score 5.735981912368849\n",
      "no_score 4.430763144035014\n",
      "yes_score 67.57240357733411\n",
      "no_score 4.852792442360719\n",
      "yes_score 19.527032357282497\n",
      "no_score -1.1341411094257863\n",
      "yes_score 41.82583982248652\n",
      "no_score 14.054544476765717\n",
      "yes_score 32.036282778951914\n",
      "no_score 17.746437544458658\n",
      "yes_score 21.52324032720528\n",
      "no_score 16.900440162223642\n",
      "yes_score 36.26793099492947\n",
      "no_score 6.144525814592106\n",
      "yes_score 7.2673961608917885\n",
      "no_score 4.153653094537746\n",
      "yes_score 46.04961995280375\n",
      "no_score 31.616966100999623\n",
      "yes_score 20.870177102836077\n",
      "no_score 17.397712508376287\n",
      "yes_score 31.27939538829472\n",
      "no_score 23.643200638763695\n",
      "yes_score 56.45644255821636\n",
      "no_score 15.756640974580089\n",
      "yes_score 51.81772112767373\n",
      "no_score 24.955456697809066\n",
      "yes_score 37.6109026310545\n",
      "no_score 35.1605543244385\n",
      "yes_score 55.11335532392544\n",
      "no_score 29.274093441614017\n",
      "yes_score 27.73224243963894\n",
      "no_score 14.462026393130632\n",
      "yes_score 15.87402493501918\n",
      "no_score 13.462118241008579\n",
      "yes_score 20.18438013620332\n",
      "no_score 21.309598947945073\n",
      "yes_score 21.877675653128932\n",
      "no_score 17.322873700169588\n",
      "yes_score 5.408094737861137\n",
      "no_score 4.335418525126879\n",
      "yes_score 25.84384985563943\n",
      "no_score 12.064454255908162\n",
      "yes_score 11.151322489490386\n",
      "no_score 3.3272004653550455\n",
      "yes_score 57.11185961541465\n",
      "no_score 22.142824524055268\n",
      "yes_score 19.704560228050415\n",
      "no_score 11.669450197421476\n",
      "yes_score 8.380645834377594\n",
      "no_score 0.26065078723498636\n",
      "yes_score 30.688274491028988\n",
      "no_score 16.928484107771293\n",
      "yes_score 19.800057120048336\n",
      "no_score -2.684826171309907\n",
      "yes_score 40.143589383636446\n",
      "no_score -5.696005985525938\n",
      "yes_score 48.990983812913704\n",
      "no_score 12.614397571029416\n",
      "yes_score 46.32639625134814\n",
      "no_score 4.837608214166885\n",
      "yes_score 17.63981043523753\n",
      "no_score 16.178651828756983\n",
      "yes_score 48.36882405024359\n",
      "no_score 15.068659881320619\n",
      "yes_score 55.54728069959944\n",
      "no_score 34.80115537227436\n",
      "yes_score 36.412962564255594\n",
      "no_score 20.989407190060554\n",
      "yes_score 18.523938247258936\n",
      "no_score 1.7030618039028285\n",
      "yes_score 60.61945898124591\n",
      "no_score 38.953746339335744\n",
      "yes_score 61.65477316295848\n",
      "no_score 35.48339845262005\n",
      "yes_score 25.33387135286903\n",
      "no_score 6.78016307482924\n",
      "yes_score 54.0497155646363\n",
      "no_score 23.85343616842387\n",
      "yes_score 35.09662752812788\n",
      "no_score 28.02301662855851\n",
      "yes_score 57.10445962669695\n",
      "no_score 23.48850260912214\n"
     ]
    }
   ],
   "source": [
    "################# filtered_vocabulary #################\n",
    "\n",
    "total_instances = 0\n",
    "num_of_correct_prediction = 0\n",
    "num_of_wrong_prediction = 0\n",
    "num_of_yes_predicted = 0\n",
    "num_of_no_predicted = 0\n",
    "positive_yes = 0\n",
    "positive_no = 0\n",
    "true_yes = 0\n",
    "true_no = 0\n",
    "\n",
    "# generate trace file based on model\n",
    "f2 = open(\"_NB-BOW-FV.txt\",\"w\") \n",
    "for i, col in df_test.iterrows():\n",
    "    f2.write(str(col['tweet_id'])+ \"  \") \n",
    "\n",
    "    # calculate the score in each class\n",
    "    document = col['text'].lower()\n",
    "    word_list = preprocess_text(document)\n",
    "    # print(word_list, \"\\n\")\n",
    "\n",
    "    yes_score = get_score(prob_class_yes, word_prob_yes_class_fv, word_list)\n",
    "    no_score = get_score(prob_class_no, word_prob_no_class_fv, word_list)\n",
    "    print(\"yes_score\", yes_score)\n",
    "    print(\"no_score\", no_score)\n",
    "\n",
    "    if yes_score > no_score:\n",
    "        num_of_yes_predicted += 1\n",
    "        f2.write(\"yes  \"+ '{:.5E}'.format(yes_score) + \"  \"+col['q1_label']+\"  \")\n",
    "        if col['q1_label'] == \"yes\":\n",
    "            f2.write(\"correct\\n\")\n",
    "        else:\n",
    "            f2.write(\"wrong\\n\")\n",
    "    else:\n",
    "        num_of_no_predicted += 1\n",
    "        f2.write(\"no  \"+ '{:.5E}'.format(no_score) + \"  \"+col['q1_label']+\"  \")\n",
    "        if col['q1_label'] == \"no\":\n",
    "            f2.write(\"correct\\n\")\n",
    "        else:\n",
    "            f2.write(\"wrong\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputting evaluation files for filterd vocabulary\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
