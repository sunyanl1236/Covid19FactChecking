{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\yilul\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\yilul\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "import copy\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility func\n",
    "\n",
    "# preprocess text\n",
    "def preprocess_text(text_str):\n",
    "    # remove punctuation and numbers\n",
    "    removed_punc_text = re.sub('[^A-Za-z]', ' ', text_str)\n",
    "    removed_punc_text_list = removed_punc_text.split()\n",
    "    # print(len(removed_punc_text_list)) #test\n",
    "\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    removed_punc_stop_text_list = [w for w in removed_punc_text_list if not w in stop_words]  \n",
    "    # print(len(removed_punc_stop_text_list)) #test\n",
    "\n",
    "    # stemming (base words)\n",
    "    stemmer = PorterStemmer()\n",
    "    for i in range(len(removed_punc_stop_text_list)):\n",
    "        removed_punc_stop_text_list[i] = stemmer.stem(removed_punc_stop_text_list[i])\n",
    "    return removed_punc_stop_text_list\n",
    "\n",
    "\n",
    "\n",
    "# calculate probability of each word in specific class\n",
    "def getProbDic(vocalbulary_dic, class_dic, total_words_num_in_class, vocabulary_length):\n",
    "    smooth_num = 0.01\n",
    "    word_prob = {} # key: word, value: probability of word, log10 format\n",
    "    for key, val in vocalbulary_dic.items():\n",
    "        fre = class_dic.get(key)\n",
    "\n",
    "        prob = 0 \n",
    "        # if word doesn't exist in yes_class_vocabulary, add 0.01 smooth\n",
    "        if fre == None:\n",
    "            prob = smooth_num / (total_words_num_in_class + vocabulary_length * smooth_num)\n",
    "        else:\n",
    "            prob = fre + smooth_num / (total_words_num_in_class + vocabulary_length * smooth_num)\n",
    "        word_prob[key] = math.log10(prob)\n",
    "        # print(word_prob[key])\n",
    "    return word_prob\n",
    "\n",
    "def cal_total_words(class_dic):\n",
    "    total_words = 0\n",
    "    for key, val in class_dic.items():\n",
    "        total_words += val\n",
    "    return total_words\n",
    "\n",
    "def get_score(prob_class, class_dic, document_list):\n",
    "    score = prob_class\n",
    "    for word in document_list:\n",
    "        if class_dic.get(word) != None:\n",
    "            score += class_dic.get(word)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7820\n",
      "5550\n",
      "2270\n"
     ]
    }
   ],
   "source": [
    "# read specific columns from csv file\n",
    "df = pd.read_csv(\"covid_training.csv\", usecols = ['tweet_id','text', 'q1_label'])\n",
    "# print(df)\n",
    "\n",
    "# count the number of documents in each class (yes/no)\n",
    "# read text in each row of training set\n",
    "yes_class_num = 0\n",
    "no_class_num = 0\n",
    "total_class_num = 0\n",
    "\n",
    "yes_class_text = \"\"\n",
    "no_class_text = \"\"\n",
    "all_text = \"\" # concatenate all texts to one string\n",
    "\n",
    "for i, col in df.iterrows():\n",
    "    if col['q1_label'].lower() == \"yes\":\n",
    "        yes_class_num += 1\n",
    "        yes_class_text += (col['text'].lower() + \" \")\n",
    "    else:\n",
    "        no_class_num += 1\n",
    "        no_class_text += (col['text'].lower() + \" \")\n",
    "    all_text += (col['text'].lower() + \" \")\n",
    "    total_class_num += 1\n",
    "\n",
    "# print(all_text) #test\n",
    "\n",
    "# probability of each class\n",
    "prob_class_yes = math.log10(yes_class_num / total_class_num)\n",
    "prob_class_no = math.log10(no_class_num / total_class_num)\n",
    "\n",
    "\n",
    "########################## preprocess all_text\n",
    "\n",
    "# map punctuation to space\n",
    "# translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) \n",
    "\n",
    "# generate vocabulary \n",
    "# count words frequency in all_text\n",
    "# remove punctuation and quotation marks\n",
    "# removed_punc_text = all_text.translate(translator).replace(\"'\",\" \").replace(\"“\",\" \").replace(\"”\",\" \").replace(\"’\",\" \").replace(\"‘\",\" \")\n",
    "all_text_list = preprocess_text(all_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################# original_vocabulary #################\n",
    "\n",
    "# calculate words frequency for original_vocabulary\n",
    "original_vocabulary = dict(Counter(all_text_list))\n",
    "# print(original_vocabulary)\n",
    "\n",
    "# count total vocabulary words in all_text based on original vocabulary\n",
    "abs_ov_length = len(original_vocabulary)\n",
    "# print(abs_ov_length)\n",
    "\n",
    "# test\n",
    "# print(original_vocabulary.get(\"the\")) # should be None\n",
    "\n",
    "# test\n",
    "abs_ov_length_dupli = cal_total_words(original_vocabulary)\n",
    "# print(abs_ov_length_dupli)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# count words frequency in yes_class_text based on original_vocabulary\n",
    "# removed_punc_yes_class_text = yes_class_text.translate(translator).replace(\"'\",\" \").replace(\"“\",\" \").replace(\"”\",\" \").replace(\"’\",\" \").replace(\"‘\",\" \")\n",
    "yes_class_text_list = preprocess_text(yes_class_text)\n",
    "\n",
    "# calculate words frequency\n",
    "yes_class_vocabulary = dict(Counter(yes_class_text_list))\n",
    "# print(yes_class_vocabulary)\n",
    "\n",
    "total_words_in_yes_class_text_ov = cal_total_words(yes_class_vocabulary)\n",
    "# print(total_words_in_yes_class_text_ov)\n",
    "\n",
    "# calculate probability of each word in class yes\n",
    "word_prob_yes_class = getProbDic(original_vocabulary, yes_class_vocabulary, total_words_in_yes_class_text_ov, abs_ov_length)\n",
    "# print(word_prob_yes_class[\"the\"]) # test\n",
    "\n",
    "\n",
    "\n",
    "# count words frequency in no_class_text based on original_vocabulary\n",
    "# removed_punc_no_class_text = no_class_text.translate(translator).replace(\"'\",\" \").replace(\"“\",\" \").replace(\"”\",\" \").replace(\"’\",\" \").replace(\"‘\",\" \")\n",
    "no_class_text_list = preprocess_text(no_class_text)\n",
    "\n",
    "# calculate words frequency\n",
    "no_class_vocabulary = dict(Counter(no_class_text_list))\n",
    "# print(no_class_vocabulary)\n",
    "\n",
    "total_words_in_no_class_text_ov = cal_total_words(no_class_vocabulary)\n",
    "# print(total_words_in_no_class_text_ov)\n",
    "\n",
    "# calculate probability of each word in class no\n",
    "word_prob_no_class = getProbDic(original_vocabulary, no_class_vocabulary, total_words_in_no_class_text_ov, abs_ov_length)\n",
    "# print(word_prob_no_class[\"the\"]) # test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'american': 15, 'best': 6, 'way': 13, 'tell': 9, 'covid': 183, 'cough': 7, 'person': 13, 'face': 8, 'wait': 4, 'test': 60, 'result': 4, 'fuck': 12, 'pleas': 26, 'follow': 10, 'govern': 18, 'instruct': 2, 'knock': 2, 'done': 6, 'feel': 8, 'like': 34, 'keep': 7, 'lose': 3, 'time': 27, 'one': 24, 'two': 10, 'kid': 5, 'direct': 3, 'corona': 91, 'viru': 85, 'disappear': 2, 'april': 2, 'actual': 7, 'suck': 2, 'someon': 7, 'spent': 3, 'hour': 2, 'protect': 12, 'move': 2, 'critic': 4, 'ill': 7, 'patient': 18, 'around': 9, 'start': 9, 'peopl': 62, 'social': 9, 'distanc': 2, 'self': 10, 'isol': 5, 'http': 187, 'co': 186, 'door': 4, 'told': 9, 'stay': 10, 'free': 4, 'month': 8, 'employ': 2, 'need': 26, 'bori': 3, 'moral': 2, 'correct': 4, 'someth': 9, 'better': 4, 'never': 10, 'hear': 10, 'anyon': 6, 'end': 7, 'worker': 14, 'gener': 6, 'food': 4, 'employe': 2, 'even': 8, 'think': 14, 'deserv': 2, 'surviv': 4, 'dr': 5, 'past': 5, 'week': 28, 'treat': 4, 'pakistan': 4, 'knew': 6, 'ppe': 3, 'today': 15, 'lost': 5, 'battl': 3, 'coronaviru': 128, 'gave': 3, 'life': 10, 'hope': 8, 'mani': 15, 'know': 23, 'name': 2, 'fun': 4, 'fact': 9, 'tradit': 3, 'spread': 47, 'fatal': 2, 'diseas': 10, 'everi': 11, 'countri': 16, 'fulli': 2, 'white': 5, 'amp': 45, 'take': 20, 'account': 8, 'devast': 2, 'live': 16, 'shit': 3, 'pass': 5, 'promis': 4, 'go': 30, 'short': 5, 'breath': 4, 'symptom': 15, 'right': 12, 'also': 10, 'give': 13, 'anxieti': 3, 'last': 9, 'pneumonia': 3, 'young': 4, 'risk': 5, 'mild': 2, 'wrong': 2, 'want': 22, 'open': 2, 'gone': 2, 'day': 24, 'icu': 3, 'phd': 2, 'whatsapp': 7, 'univers': 3, 'friend': 6, 'cool': 2, 'stock': 8, 'relief': 4, 'bill': 7, 'futur': 2, 'hey': 3, 'grandpa': 3, 'korea': 5, 'develop': 6, 'minut': 2, 'kit': 10, 'ramp': 2, 'product': 3, 'plan': 3, 'export': 2, 'rt': 11, 'suicid': 2, 'big': 9, 'room': 4, 'epidem': 4, 'happen': 6, 'come': 14, 'bad': 6, 'ever': 3, 'good': 7, 'health': 30, 'gonna': 6, 'see': 15, 'celebr': 3, 'tri': 8, 'sing': 2, 'away': 9, 'movi': 4, 'mayb': 3, 'leav': 7, 'thread': 10, 'mostli': 2, 'talk': 4, 'goe': 3, 'current': 4, 'make': 22, 'surgic': 2, 'mask': 9, 'medic': 13, 'suppli': 3, 'million': 9, 'look': 16, 'help': 19, 'possibl': 5, 'n': 9, 'e': 32, 'man': 11, 'hous': 5, 'would': 15, 'pop': 2, 'thursday': 4, 'cure': 6, 'morn': 5, 'almost': 4, 'everyth': 4, 'work': 27, 'home': 19, 'saw': 6, 'beer': 2, 'outsid': 7, 'either': 4, 'tabl': 2, 'air': 2, 'contact': 6, 'wash': 8, 'hand': 14, 'still': 14, 'caught': 3, 'thing': 13, 'understand': 5, 'year': 17, 'order': 8, 'system': 7, 'bro': 3, 'china': 21, 'read': 12, 'get': 30, 'adequ': 2, 'invit': 3, 'sometim': 2, 'ass': 5, 'miss': 5, 'without': 10, 'stop': 27, 'said': 9, 'updat': 2, 'compil': 2, 'italian': 2, 'mayor': 2, 'violat': 2, 'quarantin': 20, 'ye': 3, 'subtitl': 2, 'accur': 3, 'v': 7, 'unless': 3, 'doctor': 14, 'drug': 2, 'affect': 2, 'heart': 3, 'lead': 5, 'death': 21, 'med': 2, 'condit': 3, 'speak': 2, 'exist': 2, 'thank': 11, 'everyon': 13, 'messag': 9, 'support': 6, 'encourag': 2, 'ask': 13, 'er': 7, 'share': 13, 'doc': 5, 'brief': 5, 'fellow': 2, 'citizen': 7, 'absolut': 4, 'panic': 12, 'essenti': 2, 'etc': 2, 'avail': 2, 'centr': 2, 'state': 15, 'close': 13, 'ensur': 2, 'togeth': 3, 'fight': 14, 'creat': 5, 'india': 18, 'found': 6, 'q': 9, 'clean': 4, 'gaston': 3, 'minim': 4, 'prevent': 6, 'dysfunct': 2, 'x': 3, 'new': 20, 'region': 2, 'presid': 14, 'graduat': 2, 'parti': 8, 'send': 4, 'polic': 3, 'final': 2, 'sensibl': 2, 'advic': 5, 'nurs': 5, 'seen': 4, 'lot': 4, 'recommend': 3, 'avoid': 5, 'first': 17, 'place': 4, 'real': 7, 'insid': 6, 'chang': 5, 'dad': 2, 'p': 4, 'z': 2, 'folk': 4, 'love': 3, 'call': 22, 'millenni': 3, 'break': 7, 'yell': 2, 'boomer': 2, 'parent': 2, 'sit': 2, 'fear': 6, 'guy': 7, 'happi': 2, 'wow': 3, 'world': 16, 'organ': 2, 'warn': 7, 'could': 11, 'put': 16, 'hospit': 18, 'kill': 8, 'sick': 15, 'differ': 3, 'els': 5, 'much': 6, 'video': 6, 'spain': 4, 'watch': 8, 'cancel': 5, 'check': 8, 'hard': 4, 'child': 3, 'came': 5, 'might': 3, 'remind': 10, 'cdc': 3, 'announc': 8, 'via': 2, 'stori': 6, 'challeng': 4, 'save': 3, 'crisi': 11, 'voic': 4, 'healthcar': 4, 'pay': 4, 'statu': 2, 'servic': 2, 'aggress': 3, 'issu': 4, 'h': 2, 'mom': 3, 'g': 5, 'bt': 2, 'son': 2, 'min': 3, 'continu': 7, 'black': 3, 'rona': 2, 'may': 7, 'taken': 3, 'vega': 3, 'empti': 2, 'tbh': 2, 'nasti': 2, 'flu': 20, 'went': 5, 'round': 3, 'earli': 8, 'jan': 4, 'ppl': 4, 'includ': 5, 'due': 11, 'thought': 5, 'twitter': 6, 'prom': 3, 'th': 3, 'march': 9, 'interact': 2, 'join': 4, 'yet': 6, 'war': 2, 'politician': 4, 'act': 2, 'itali': 8, 'rememb': 3, 'job': 3, 'compani': 5, 'regard': 2, 'global': 9, 'situat': 3, 'gummi': 2, 'ago': 11, 'entir': 7, 'popul': 3, 'bitch': 5, 'hi': 2, 'origin': 6, 'back': 10, 'mean': 3, 'lq': 2, 'eat': 2, 'pure': 3, 'built': 2, 'epidemiologist': 2, 'say': 24, 'trump': 31, 'handl': 6, 'irrespons': 3, 'elect': 3, 'offici': 12, 'use': 7, 'among': 2, 'public': 17, 'expert': 10, 'approach': 2, 'top': 3, 'k': 8, 'case': 48, 'worldwid': 3, 'senat': 3, 'trade': 4, 'immedi': 4, 'resign': 4, 'word': 2, 'donat': 10, 'idk': 2, 'small': 2, 'alway': 5, 'park': 2, 'spot': 2, 'profession': 3, 'hero': 3, 'line': 4, 'alreadi': 8, 'front': 5, 'us': 32, 'dear': 12, 'allah': 2, 'cost': 7, 'show': 7, 'long': 3, 'fat': 3, 'took': 4, 'dirti': 2, 'racist': 4, 'bodi': 2, 'beat': 2, 'lord': 2, 'wear': 4, 'hit': 4, 'find': 3, 'infect': 19, 'harvey': 2, 'germani': 3, 'total': 3, 'ventil': 2, 'made': 5, 'govt': 2, 'bed': 2, 'rate': 7, 'tini': 2, 'bco': 2, 'mass': 3, 'allow': 5, 'fewer': 2, 'non': 2, 'report': 15, 'age': 5, 'die': 27, 'believ': 6, 'la': 6, 'nation': 7, 'indiafightscorona': 2, 'increas': 3, 'realdonaldtrump': 22, 'shut': 3, 'scienc': 6, 'evid': 3, 'polit': 6, 'older': 2, 'great': 7, 'coupl': 2, 'whatev': 2, 'next': 4, 'admit': 8, 'whole': 2, 'within': 2, 'dead': 2, 'school': 6, 'legitim': 2, 'huge': 2, 'provid': 5, 'strong': 2, 'desir': 2, 'return': 9, 'function': 2, 'economi': 3, 'societi': 3, 'citi': 5, 'corpor': 2, 'bailout': 2, 'suffici': 2, 'care': 14, 'class': 7, 'famili': 7, 'focu': 3, 'street': 2, 'proper': 2, 'treatment': 3, 'got': 9, 'old': 9, 'commun': 4, 'learn': 4, 'director': 2, 'allegedli': 8, 'posit': 9, 'refus': 10, 'priorit': 3, 'respect': 3, 'clear': 3, 'sen': 4, 'illeg': 2, 'inform': 13, 'buy': 3, 'profit': 2, 'texa': 2, 'ny': 3, 'largest': 2, 'fake': 35, 'question': 4, 'tweet': 11, 'rumor': 14, 'let': 12, 'news': 23, 'guard': 2, 'respons': 11, 'link': 15, 'stuff': 2, 'natur': 2, 'larg': 2, 'sourc': 6, 'control': 4, 'page': 2, 'misinform': 4, 'trust': 5, 'republican': 4, 'implic': 2, 'sell': 2, 'hoax': 28, 'fox': 4, 'claim': 6, 'receipt': 2, 'committe': 3, 'danger': 4, 'note': 4, 'imag': 2, 'platform': 4, 'caus': 7, 'disregard': 3, 'pm': 6, 'local': 4, 'wuhan': 5, 'light': 3, 'number': 11, 'reportedli': 5, 'uk': 5, 'border': 2, 'johnson': 2, 'shutdown': 2, 'refer': 2, 'polici': 2, 'failur': 2, 'leadership': 5, 'gop': 2, 'outbreak': 11, 'blame': 5, 'busi': 5, 'enough': 8, 'chief': 5, 'staff': 4, 'kyari': 2, 'suspect': 2, 'contract': 2, 'serious': 3, 'sinc': 9, 'trip': 4, 'ron': 3, 'paul': 4, 'publish': 3, 'articl': 4, 'rumour': 3, 'less': 4, 'list': 3, 'onlin': 2, 'flight': 3, 'sure': 4, 'info': 4, 'hold': 4, 'daili': 5, 'cdcgov': 3, 'latest': 5, 'step': 2, 'leader': 2, 'pandem': 7, 'america': 5, 'other': 5, 'fill': 2, 'sorri': 3, 'noth': 5, 'seriou': 5, 'bigot': 2, 'delhi': 2, 'indian': 5, 'woman': 4, 'belong': 2, 'area': 7, 'north': 2, 'night': 2, 'u': 6, 'worri': 2, 'spoke': 2, 'photo': 3, 'circul': 3, 'fals': 9, 'detail': 5, 'confirm': 14, 'nigeria': 3, 'paid': 3, 'c': 3, 'congress': 4, 'propaganda': 2, 'shortag': 2, 'student': 3, 'occur': 2, 'viewer': 2, 'action': 6, 'conspiraci': 5, 'realli': 9, 'januari': 5, 'lv': 2, 'human': 2, 'pl': 2, 'forward': 2, 'anim': 2, 'saliva': 2, 'educ': 3, 'urg': 2, 'spray': 3, 'throughout': 2, 'nytim': 2, 'explain': 2, 'fakenew': 8, 'pidi': 2, 'quot': 2, 'chines': 10, 'communist': 4, 'purpos': 2, 'activ': 2, 'push': 2, 'theori': 3, 'campaign': 3, 'unit': 7, 'singl': 3, 'patna': 5, 'problem': 5, 'qatar': 13, 'st': 3, 'senior': 2, 'hiv': 2, 'aid': 2, 'immun': 2, 'scam': 2, 'upon': 2, 'letter': 2, 'post': 2, 'clearli': 2, 'far': 4, 'standard': 2, 'hurt': 2, 'nobodi': 2, 'viral': 4, 'media': 16, 'demand': 2, 'impact': 2, 'w': 13, 'must': 4, 'second': 2, 'unintent': 2, 'feder': 3, 'sever': 3, 'respiratori': 3, 'j': 5, 'justic': 2, 'parliament': 2, 'emerg': 11, 'rule': 2, 'suspend': 2, 'prison': 2, 'airlin': 2, 'instead': 7, 'shame': 3, 'hate': 2, 'well': 6, 'reason': 2, 'anoth': 6, 'peddl': 2, 'vegetarian': 4, 'date': 2, 'uganda': 2, 'true': 4, 'team': 2, 'fraud': 2, 'alert': 5, 'taxpay': 2, 'websit': 2, 'impos': 3, 'financi': 3, 'advis': 2, 'drtedro': 3, 'full': 4, 'extent': 2, 'disast': 2, 'fine': 5, 'ban': 3, 'uae': 3, 'heard': 4, 'recov': 3, 'game': 2, 'wish': 3, 'pibfactcheck': 2, 'releas': 2, 'tie': 3, 'threaten': 2, 'comment': 2, 'request': 2, 'delet': 2, 'democrat': 3, 'soon': 5, 'expos': 3, 'awar': 4, 'blood': 2, 'forev': 2, 'stain': 2, 'precaut': 4, 'sensation': 2, 'safe': 4, 'lie': 6, 'gather': 2, 'donald': 3, 'liber': 2, 'answer': 2, 'repeat': 2, 'distribut': 3, 'gen': 2, 'attend': 2, 'hotel': 3, 'decid': 7, 'period': 4, 'disinform': 2, 'hatr': 2, 'season': 6, 'turn': 4, 'fail': 6, 'cnn': 2, 'tapper': 2, 'play': 3, 'malaysian': 2, 'b': 3, 'quiet': 2, 'cannot': 3, 'arrest': 2, 'bihar': 5, 'travel': 17, 'histori': 5, 'aiim': 5, 'foreign': 2, 'prepar': 4, 'migrant': 2, 'singapor': 3, 'south': 4, 'africa': 2, 'saif': 2, 'ali': 2, 'reach': 2, 'member': 3, 'scare': 6, 'oh': 2, 'coronavirusupd': 3, 'rwanda': 2, 'bring': 2, 'usa': 3, 'fifacom': 3, 'accept': 3, 'fifaworldcup': 3, 'fr': 2, 'amnesti': 2, 'milan': 2, 'custom': 2, 'money': 4, 'glove': 3, 'plastic': 2, 'bag': 2, 'tonight': 6, 'collect': 4, 'safeti': 5, 'victim': 3, 'suggest': 2, 'combin': 3, 'sar': 5, 'damag': 4, 'lung': 5, 'thespybrief': 2, 'nationalnurs': 2, 'statement': 5, 'facil': 2, 'west': 2, 'neg': 3, 'album': 3, 'box': 2, 'cover': 3, 'build': 2, 'deadli': 3, 'incompet': 5, 'administr': 6, 'massiv': 2, 'potu': 5, 'contain': 12, 'friday': 2, 'coronavirusoutbreak': 5, 'vaccin': 3, 'f': 2, 'lack': 6, 'derelict': 3, 'duti': 3, 'spew': 2, 'downplay': 3, 'unaccept': 2, 'rick': 3, 'market': 2, 'estim': 2, 'assist': 3, 'secretari': 2, 'probabl': 2, 'asian': 8, 'individu': 3, 'transmiss': 3, 'toilet': 2, 'paper': 3, 'ff': 2, 'center': 2, 'vs': 2, 'becom': 2, 'catch': 2, 'data': 2, 'research': 5, 'retweet': 3, 'meghan': 2, 'joke': 2, 'accord': 4, 'tactic': 2, 'vp': 2, 'penc': 5, 'sneez': 3, 'across': 3, 'gt': 2, 'preval': 2, 'appar': 2, 'measur': 4, 'benefit': 4, 'franc': 5, 'enter': 2, 'tiktok': 4, 'least': 4, 'racism': 2, 'touch': 3, 'stake': 2, 'ticket': 2, 'vietnam': 2, 'four': 2, 'consult': 2, 'translat': 2, 'cold': 3, 'clip': 2, 'coronviru': 2, 'write': 2, 'bunch': 2, 'deadlier': 3, 'attempt': 2, 'enemi': 3, 'requir': 2, 'effort': 3, 'combat': 4, 'ir': 2, 'holi': 3, 'insur': 2, 'doh': 2, 'initi': 2, 'approx': 3, 'dynasti': 2, 'point': 2, 'hongkong': 2, 'listen': 2, 'littl': 2, 'clinic': 4, 'offic': 4, 'seattl': 2, 'confront': 3, 'fund': 2, 'privat': 2, 'statedept': 2, 'houseforeign': 2, 'secpompeo': 2, 'deni': 5, 'strang': 2, 'iran': 3, 'japan': 2, 'hunch': 2, 'enabl': 2, 'empir': 2, 'visit': 3, 'known': 2, 'incl': 2, 'jordan': 2, 'russia': 2, 'wors': 2, 'kag': 2, 'three': 2, 'wonder': 4, 'desist': 2, 'ag': 2, 'salari': 4, 'hhsgov': 2, 'herea': 4, 'road': 2, 'crash': 2, 'chronic': 2, 'novel': 2, 'henc': 2, 'minist': 3, 'israel': 4, 'netanyahu': 3, 'isra': 3, 'namast': 2, 'coronaoutbreak': 3, 'coronaalert': 2, 'diagnos': 3, 'ebola': 3, 'effect': 2, 'associ': 3, 'access': 2, 'fisa': 2, 'peoplea': 2, 'wona': 2, 'quit': 2, 'coronavirusindia': 2, 'apolog': 2, 'excel': 2, 'mr': 2, 'doesna': 3, 'amaz': 2, 'feb': 3, 'wake': 2, 'freak': 2, 'sanit': 2, 'cheap': 2, 'arriv': 2, 'incub': 2, 'throat': 2, 'cpac': 2, 'attende': 3, 'gov': 3, 'part': 2, 'kind': 2, 'pag': 2, 'seem': 3, 'focus': 2, 'taal': 2, 'cov': 2, 'youa': 2, 'ng': 2, 'na': 2, 'chloroquin': 2, 'anti': 3}\n6098\n"
     ]
    }
   ],
   "source": [
    "################# filtered_vocabulary #################\n",
    "# calculate words frequency for filtered_vocabulary\n",
    "filtered_vocabulary = {key:val for key, val in original_vocabulary.items() if val != 1}\n",
    "print(filtered_vocabulary)\n",
    "\n",
    "# count total vocabulary words in all_text based on filtered vocabulary\n",
    "abs_fv_length = len(filtered_vocabulary)\n",
    "\n",
    "# test\n",
    "abs_fv_length_dupli = cal_total_words(filtered_vocabulary)\n",
    "print(abs_fv_length_dupli)\n",
    "\n",
    "# calculate class_dic based on filtered_vocabulary\n",
    "yes_class_vocabulary_fv = copy.deepcopy(yes_class_vocabulary)\n",
    "for key in yes_class_vocabulary.keys():\n",
    "    # remove all words that doesn't exist in filtered_vocabulary\n",
    "    if filtered_vocabulary.get(key) == None:\n",
    "        del yes_class_vocabulary_fv[key]\n",
    "\n",
    "\n",
    "total_words_in_yes_class_text_fv = cal_total_words(yes_class_vocabulary_fv)\n",
    "# print(total_words_in_yes_class_text_fv)\n",
    "\n",
    "# calculate probability of each word in class yes\n",
    "word_prob_yes_class_fv = getProbDic(filtered_vocabulary, yes_class_vocabulary_fv, total_words_in_yes_class_text_fv, abs_fv_length)\n",
    "\n",
    "\n",
    "\n",
    "# calculate class_dic based on filtered_vocabulary\n",
    "no_class_vocabulary_fv = copy.deepcopy(no_class_vocabulary)\n",
    "for key in no_class_vocabulary.keys():\n",
    "    # remove all words that doesn't exist in filtered_vocabulary\n",
    "    if filtered_vocabulary.get(key) == None:\n",
    "        del no_class_vocabulary_fv[key]\n",
    "\n",
    "\n",
    "total_words_in_no_class_text_fv = cal_total_words(no_class_vocabulary_fv)\n",
    "# print(total_words_in_no_class_text_fv)\n",
    "\n",
    "# calculate probability of each word in class no\n",
    "word_prob_no_class_fv = getProbDic(filtered_vocabulary, no_class_vocabulary_fv, total_words_in_no_class_text_fv, abs_fv_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "yes_score 2.000595227066772\nno_score -6.712027963585733\nyes_score 4.092754624038468\nno_score -2.693935262534704\nyes_score 20.01355654987832\nno_score -5.013177559247103\nyes_score 11.187859863276786\nno_score -20.443345793894615\nyes_score 3.6210290335543154\nno_score -8.879010891325217\nyes_score 3.5513938891481356\nno_score -8.657160503124809\nyes_score -1.8243444561648654\nno_score -8.754070894266306\nyes_score 5.380699764521687\nno_score -23.88332581321126\nyes_score 7.320418108182008\nno_score -16.400939758921538\nyes_score -4.114044436741834\nno_score -0.15422900819191643\nyes_score 3.6948158029132103\nno_score -9.356128994922122\nyes_score 2.7337322280706915\nno_score 1.8433224432088282\nyes_score 13.634335639179945\nno_score -2.5655629234679767\nyes_score 7.018968684303738\nno_score -0.7009390387307473\nyes_score -3.653646575688291\nno_score -8.352584581351149\nyes_score 15.24469795273427\nno_score -23.534880844647027\nyes_score 0.3059887239641659\nno_score 3.5422941491539763\nyes_score 3.3894202098188564\nno_score 1.3590267323532808\nyes_score 8.378424867926165\nno_score -0.22381735072837716\nyes_score 4.629629896710443\nno_score -2.174772148516406\nyes_score 7.047000205525503\nno_score -23.224784629892067\nyes_score 1.7917244625071445\nno_score 1.1936564857660663\nyes_score 12.494828854002938\nno_score 3.1719185829557883\nyes_score 5.687974919952924\nno_score 3.978985784570985\nyes_score 6.41744999377502\nno_score -1.4808710762231285\nyes_score 3.2600734497374892\nno_score -3.7011101283174694\nyes_score 3.864998212094876\nno_score -4.026039879916787\nyes_score 2.218627047089772\nno_score -5.51652076196705\nyes_score 5.087798767740683\nno_score -9.232973211820571\nyes_score 3.4449377889193737\nno_score -9.055101835268418\nyes_score 3.5876040967211527\nno_score 2.978984461097355\nyes_score -5.3254180621497955\nno_score 4.774783451001711\nyes_score -3.3112257475102007\nno_score 1.0579965476195337\nyes_score 1.9678155917762554\nno_score 1.4946855360939746\nyes_score 7.018968684303738\nno_score -0.7009390387307473\nyes_score 1.888634076810734\nno_score 1.3662005582635166\nyes_score 13.321644616780354\nno_score -8.868732957585383\nyes_score 3.8129139283715303\nno_score 3.688420816342693\nyes_score 5.690450380340428\nno_score -1.5240574276662076\nyes_score 4.356982278647376\nno_score 2.9225068403817263\nyes_score 4.296875794241521\nno_score -14.240175667918036\nyes_score 17.602098981994374\nno_score -1.5143853830742326\nyes_score -1.6994043676133659\nno_score -9.356125213573714\nyes_score 10.389319813202619\nno_score -5.5609412032089045\nyes_score 3.5876040967211527\nno_score 2.978984461097355\nyes_score 10.872477420157704\nno_score -4.923973169266309\nyes_score -4.43633382935041\nno_score -7.088629100070568\nyes_score 10.939032305869892\nno_score 6.8469657693356485\nyes_score 4.064725496801862\nno_score -3.216814543506924\nyes_score 8.56110743978653\nno_score -8.17872036331722\nyes_score 8.384108161221013\nno_score -11.213928359026154\nyes_score 6.5929995652677835\nno_score -1.5355713760680032\nyes_score 2.7019972221218373\nno_score -21.762752550423595\nyes_score 8.645678573641412\nno_score 5.90477763751834\nyes_score 7.565512227866641\nno_score -1.4764436152555518\n"
     ]
    }
   ],
   "source": [
    "################# original_vocabulary #################\n",
    "\n",
    "# read specific columns from csv file\n",
    "df_test = pd.read_csv(\"covid_test_public.csv\", usecols = ['tweet_id','text', 'q1_label'])\n",
    "\n",
    "\n",
    "\n",
    "# generate trace file based on model\n",
    "f = open(\"_NB-BOW-OV.txt\",\"w\") \n",
    "for i, col in df_test.iterrows():\n",
    "    f.write(str(col['tweet_id'])+ \"  \") \n",
    "\n",
    "    # calculate the score in each class\n",
    "    document = col['text'].lower()\n",
    "    # print(document+\"\\n\")\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) \n",
    "    removed_punc_document = document.translate(translator).replace(\"'\",\" \").replace(\"“\",\" \").replace(\"”\",\" \").replace(\"’\",\" \").replace(\"‘\",\" \")\n",
    "    word_list = removed_punc_document.split()\n",
    "    # print(word_list, \"\\n\")\n",
    "\n",
    "    yes_score = get_score(prob_class_yes, word_prob_yes_class, word_list)\n",
    "    no_score = get_score(prob_class_no, word_prob_no_class, word_list)\n",
    "    print(\"yes_score\", yes_score)\n",
    "    print(\"no_score\", no_score)\n",
    "\n",
    "    if yes_score > no_score:\n",
    "        f.write(\"yes  \"+ '{:.5E}'.format(yes_score) + \"  \"+col['q1_label']+\"  \")\n",
    "        if col['q1_label'] == \"yes\":\n",
    "            f.write(\"correct\\n\")\n",
    "        else:\n",
    "            f.write(\"wrong\\n\")\n",
    "    else:\n",
    "        f.write(\"no  \"+ '{:.5E}'.format(no_score) + \"  \"+col['q1_label']+\"  \")\n",
    "        if col['q1_label'] == \"no\":\n",
    "            f.write(\"correct\\n\")\n",
    "        else:\n",
    "            f.write(\"wrong\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "yes_score 7.746997602352106\nno_score -1.2291975664797716\nyes_score 4.092754779735124\nno_score -2.572270496174484\nyes_score 20.01355692675082\nno_score -4.648181818376251\nyes_score 11.187857083561257\nno_score 1.122986994909096\nyes_score 3.6210283706063997\nno_score -3.396179529015383\nyes_score 3.5513924488490973\nno_score 2.0651735749774063\nyes_score -1.7195948271552637\nno_score -8.510741310438773\nyes_score 5.380699888858835\nno_score -18.0354997675694\nyes_score 7.320416103322234\nno_score -0.19577424188362857\nyes_score 1.7371080776837673\nno_score -0.03256576886294127\nyes_score 3.694814300276452\nno_score 1.3662055722105948\nyes_score 2.7337322601142984\nno_score 1.8433226569936079\nyes_score 13.634335065635891\nno_score 2.9172692786396155\nyes_score 7.0189679237258495\nno_score 4.660228277491001\nyes_score -3.54889748683808\nno_score -2.8697524806059462\nyes_score 15.244697091342255\nno_score -12.325887318670361\nyes_score 0.4107381924861149\nno_score 3.54229491310039\nyes_score 3.3894204734005813\nno_score 1.3590282809541676\nyes_score 8.378425143075528\nno_score -0.10215208042621038\nyes_score 4.629629422352933\nno_score 3.186395586688321\nyes_score 7.046997234662426\nno_score -1.6584520792349515\nyes_score 1.791724572960409\nno_score 1.1936571119643684\nyes_score 12.49482844797351\nno_score 8.533087134185667\nyes_score 5.687975001761199\nno_score 3.9789862374051075\nyes_score 6.417449555458896\nno_score 3.880297134178495\nyes_score 3.2600730209889988\nno_score 1.6600580264213491\nyes_score 9.611401521178658\nno_score -3.782712132039818\nyes_score 2.3233753566300717\nno_score 5.205815261223996\nyes_score 5.087797370245121\nno_score 1.4893615490566319\nyes_score 3.4449371299049822\nno_score -3.5722703710770394\nyes_score 3.5876041026686414\nno_score 2.978984486028087\nyes_score 0.5257344191002171\nno_score 4.774782177105953\nyes_score 2.435177369283253\nno_score 1.0579955331268327\nyes_score 1.9678156668278581\nno_score 1.494685856648676\nyes_score 7.0189679237258495\nno_score 4.660228277491001\nyes_score 1.8886340785100164\nno_score 1.3662005682846936\nyes_score 13.321644483292394\nno_score -3.2642347590962824\nyes_score 3.81291408432109\nno_score 3.6884211378313436\nyes_score 5.690449700488445\nno_score 3.8371098908084873\nyes_score 4.356982518043763\nno_score 2.9225080729822865\nyes_score 4.296875282734185\nno_score -8.635679213061424\nyes_score 17.602098383754857\nno_score 3.968447576034839\nyes_score 4.046998181190757\nno_score -3.873294111835446\nyes_score 10.389320291192051\nno_score -5.317609972818552\nyes_score 3.5876041026686414\nno_score 2.978984486028087\nyes_score 10.872476912020106\nno_score 0.5588597668198085\nyes_score 12.802873146916458\nno_score 8.994866826527678\nyes_score 10.939032401768516\nno_score 6.846966589559962\nyes_score 4.064725542399268\nno_score -3.0951499503458275\nyes_score 8.561107109450251\nno_score 2.665279191090007\nyes_score 8.38410780016511\nno_score -5.609431212904188\nyes_score 6.592999690322335\nno_score -1.4139061588829143\nyes_score 8.448398074788876\nno_score -5.435924850837386\nyes_score 8.645678822422072\nno_score 5.90477872918475\nyes_score 7.565512698477434\nno_score -1.3547763583566397\n"
     ]
    }
   ],
   "source": [
    "################# filtered_vocabulary #################\n",
    "\n",
    "# generate trace file based on model\n",
    "f2 = open(\"_NB-BOW-FV.txt\",\"w\") \n",
    "for i, col in df_test.iterrows():\n",
    "    f2.write(str(col['tweet_id'])+ \"  \") \n",
    "\n",
    "    # calculate the score in each class\n",
    "    document = col['text'].lower()\n",
    "    # print(document+\"\\n\")\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) \n",
    "    removed_punc_document = document.translate(translator).replace(\"'\",\" \").replace(\"“\",\" \").replace(\"”\",\" \").replace(\"’\",\" \").replace(\"‘\",\" \")\n",
    "    word_list = removed_punc_document.split()\n",
    "    # print(word_list, \"\\n\")\n",
    "\n",
    "    yes_score = get_score(prob_class_yes, word_prob_yes_class_fv, word_list)\n",
    "    no_score = get_score(prob_class_no, word_prob_no_class_fv, word_list)\n",
    "    print(\"yes_score\", yes_score)\n",
    "    print(\"no_score\", no_score)\n",
    "\n",
    "    if yes_score > no_score:\n",
    "        f2.write(\"yes  \"+ '{:.5E}'.format(yes_score) + \"  \"+col['q1_label']+\"  \")\n",
    "        if col['q1_label'] == \"yes\":\n",
    "            f2.write(\"correct\\n\")\n",
    "        else:\n",
    "            f2.write(\"wrong\\n\")\n",
    "    else:\n",
    "        f2.write(\"no  \"+ '{:.5E}'.format(no_score) + \"  \"+col['q1_label']+\"  \")\n",
    "        if col['q1_label'] == \"no\":\n",
    "            f2.write(\"correct\\n\")\n",
    "        else:\n",
    "            f2.write(\"wrong\\n\")\n",
    "\n",
    "f.close()"
   ]
  }
 ]
}